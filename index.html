<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Tan Wang</title>
  
  <meta name="author" content="Tan Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="image/favicon_ntu.ico">
</head>

  <body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Tan Wang &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		  <img style="vertical-align:middle" src='image/wangtan_name.gif' height='50px' width='WIDTHpx'>
		  </name>
        </p>
		<p>Currently, Tan Wang is a first-year Ph.D. student at <a href="https://mreallab.github.io/">MreaL Lab</a> of Nanyang Technological University (NTU), supervised by <a href="https://www.ntu.edu.sg/home/hanwangzhang/">Prof. Zhang Hanwang</a>. 
		His research interests include but not limit to Visual Reasoning, Causal Inference and Vision & Language.
		</p>
		Before that, He obtained the honoured bachelor degree in <a href="http://www.sice.uestc.edu.cn/">Department of EIE</a> from <a href="https://en.uestc.edu.cn/">University of Electronic Science and Technology of China (UESTC)</a> in 2020. 
		He was a research assistant at <a href="http://cfm.uestc.edu.cn">Center for Future Media </a>, supervised by  Prof. <a href="https://interxuxing.github.io/">Xing Xu</a> and Prof. <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>. 
		He also had a close research collaboration with Prof. <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a> at TU Delft.
        </p>


        <p align=center>
          <a href="mailto:wangt97@hotmail.com">Email</a> &nbsp/&nbsp
          <a href="image/cv_wangtan.pdf">CV</a> &nbsp/&nbsp
          <a href="https://github.com/Wangt-CN">Github</a> 

        </p>

        </td>
        <td width="33%">
        <img src="image/wangtan3.jpg" width="210">
        </td>
      </tr>
      </table>


<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>News</heading>
          <p>
		  <li> <strongsmall>[2020/04]</strongsmall> &nbsp;&nbsp;<smalll>2 Journal papers are accepted by TNNLS 2020.</smalll><br/>
		  <li> <strongsmall>[2020/02]</strongsmall> &nbsp;&nbsp;<smalll>1 paper with Prof. Hanwang Zhang is accepted by CVPR 2020.</smalll><br/>
          <li> <strongsmall>[2019/07]</strongsmall> &nbsp;&nbsp;<smalll>1 paper with Prof. Alan Hanjalic is accepted by ACM MM 2019 Oral.</smalll><br/>
          <!--
	  <li> <strongsmall>[2019/03/23]</strongsmall> &nbsp;&nbsp;<smalll>1 paper with Prof. Alan Hanjalic submitted to ICCV19.</smalll><br/>
          <li> <strongsmall>[2019/02/25]</strongsmall>&nbsp;&nbsp; <smalll>CVPR boardline reject. Better than AAAI. Revise it and try again! </smalll><br/>
          <li> <strongsmall>[2018/12/24]</strongsmall> &nbsp;&nbsp;<smalll>1 revised paper (from AAAI19) submitted to TNNLS. </smalll><br/>
          <li> <strongsmall>[2018/11/13]</strongsmall> &nbsp;&nbsp;<smalll>1 new paper submitted to CVPR19. </smalll><br/>
          <li> <strongsmall>[2018/11/01]</strongsmall> &nbsp;&nbsp;<smalll>My fisrt paper is regected by AAAI19 :(  Keep going! </smalll><br/>
          <li> <strongsmall>[2018/09/08]</strongsmall> &nbsp;&nbsp;<smalll>My fisrt paper submitted to AAAI19.</smalll><br/> 
          -->
          
          </p>
        </td>
      </tr>
</table>




      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Education</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <img src='image/uestc_icon.jpg' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>University of Electronic Science and Technology of China (UESTC), China</stronghuge><br />
          Honours Degree in Electronic Information Engineering &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2016 - Jun. 2020 <br />
          GPA: <strong>92.98</strong>/100, &nbsp;&nbsp;Ranking: <strong>2/284</strong> (Overall) or <strong>1/415</strong> (first 2 years)<br />
          Supervisors: Prof. <a href="https://interxuxing.github.io/">Xing Xu</a> and Prof. <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>.  &nbsp;&nbsp; Collaborated with Prof. <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a>
          </p>
        </td>
      </tr>


        <tr>
          <td width="10%">
            <img src='image/chiba_icon.png' width="105">
          </td>

          <td width="90%" valign="middle">
          <p>
          <stronghuge>Chiba University, Japan</stronghuge><br />
          Exchange Program &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull;  Aug. 2017 <br />
          <a href="http://www.jst.go.jp/crcc/ssc/">Sakura Science Club Scholarship</a> awardee. Funded by Japan Science and Technology Agency <a href="http://www.jst.go.jp/EN/index.html">(JST)</a>.
          
          </p>
        </td>
      </tr>
	  
	  
	    <tr>
          <td width="10%">
            <img src='image/ntu_icon.jpg' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>Nanyang Technological University (NTU), Singapore</stronghuge><br />
          First-year Ph.D. in <a href="https://mreallab.github.io/">MreaL Lab</a>, School of Computer Science and Engineering &nbsp;&nbsp;&nbsp;&nbsp; &bull; Aug. 2020 - Present <br />
          Supervisor: Prof. <a href="https://www.ntu.edu.sg/home/hanwangzhang/">Zhang Hanwang</a>
          </p>
        </td>
      </tr>
	  
      </table>





<p></p><p></p><p></p><p></p><p></p>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research Experience</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <a href="http://cfm.uestc.edu.cn/">
            <img src='image/cfm_icon4.png' width="100">
            </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge>Center For Future Media, UESTC</stronghuge><br />
          <huge><em>Research  Assistant</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Mar. 2018 - Jun. 2020 <br />
          Advisors: &nbsp; Prof. <a href="https://interxuxing.github.io/">Xing Xu</a> and Prof. <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>.  &nbsp;&nbsp;Collaborated with  Prof. <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a><br/>
          <li> Proposed several novel methods for cross-modal retrieval which achieves the state-of-the-art performance on image-text matching.<br/>
          <li> Combined the GCN with Visual Question Generation Task and further boost the performance on an unexplored challenging task zero-shot VQA. <br/>
          <li> Complete 3 works and make the submission.
          </p>
        </td>
      </tr>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <a href="https://mreallab.github.io/people.html">
            <img src='image/mreal_icon.png' width="100">
          </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge>MReal Lab, NTU</stronghuge><br />
          <huge><em>Research  Assistant</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; July. 2019 - Aug. 2020 <br />
          Advisors: &nbsp; Prof. <a href="https://www.ntu.edu.sg/home/hanwangzhang/">Hanwang Zhang </a><!--
          <li> Proposed several novel methods for cross-modal retrieval which achieves the state-of-the-art performance on image-text matching.<br/>
          <li> Comnined the GCN with Visual Question Generation Task and further boost the performance on an unexplored challenging task zero-shot VQA. <br/>
          <li> Complete 3 works and make the submission.-->
          </p>
        </td>
      </tr>
  


<p></p><p></p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publication & Manuscript</heading>
        </td>
      </tr>
      </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/vc-rcnn/framework_github.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">
      <!--
    <a href="#">
            <papertitle>Cross-stream Selective Networks for Action Recognition</papertitle>
    </a>
    <br>
     -->
	 <strong>Visual Commonsense R-CNN</strong><br>
       <strong>Tan Wang</strong>,
    <a>Jianqiang Huang</a>,
    <a href="https://www.ntu.edu.sg/home/hanwangzhang/">Hanwang Zhang</a>,
      <a href="https://qianrusun.com/">Qianru Sun</a> <br>
        <em>IEEE International Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2020</strong>, <a href="https://arxiv.org/abs/2002.12204"><strong>[Paperlink]</strong></a>, 
		<a href="https://github.com/Wangt-CN/VC-R-CNN"><strong>[Code]</strong></a>, <a href="https://zhuanlan.zhihu.com/p/111306353"><strong>[Zhihu]</strong></a></em><br>
        <em>Area: Visual and Language, Causal Reasoning, Self-supervised Learning</em> <br>
        <p></p>
		<p>In this paper, we present a novel un-/self-supervised feature representation learning method, Visual Commonsense Region-based Convolutional Neural Network (VC R-CNN), to serve as an improved visual region encoder for Vision & Language high-level tasks. </p>
      </td>
    </tr>
   </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/vc-rcnn/VC_CVPRW.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">
      <!--
    <a href="#">
            <papertitle>Cross-stream Selective Networks for Action Recognition</papertitle>
    </a>
    <br>
     -->
	 <strong>Visual Commonsense Representation Learning via Causal Inference (Abstact Version of VC R-CNN)</strong><br>
       <strong>Tan Wang</strong>,
    <a>Jianqiang Huang</a>,
    <a href="https://www.ntu.edu.sg/home/hanwangzhang/">Hanwang Zhang</a>,
      <a href="https://qianrusun.com/">Qianru Sun</a> <br>
        <em>IEEE International Conference on Computer Vision and Pattern Recognition MVM Workshop, <strong>CVPRW 2020</strong>, <a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w26/Wang_Visual_Commonsense_Representation_Learning_via_Causal_Inference_CVPRW_2020_paper.html"><strong>[Paperlink]</strong></a>, <a href="https://github.com/Wangt-CN/VC-R-CNN"><strong>[Code]</strong></a>, <a href="https://zhuanlan.zhihu.com/p/111306353"><strong>[Zhihu]</strong></a></em><br>
        <em><strong><font color="#a82e2e">(Oral Presentation)</font></strong></em> <br>
		<em>Area: Visual and Language, Causal Reasoning, Self-supervised Learning</em> <br>
      </td>
    </tr>
   </table>



    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/mtfn.png'  width="200" height="130">
      </td>
      <td valign="top" width="75%">
      <!--
    <a href="#">
            <papertitle>Cross-stream Selective Networks for Action Recognition</papertitle>
    </a>
    <br>
     -->
	 <strong>Matching Images and Text with Multi-modal Tensor Fusion and Re-ranking</strong><br>
       <strong>Tan Wang</strong>,
    <a href="https://interxuxing.github.io/">Xing Xu</a>,
    <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
      <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a>,
      <a href="http://cfm.uestc.edu.cn/~shenht/">Heng Tao Shen</a> <br>
        <em>ACM International Conference on Multimedia, <strong>MM 2019</strong>, Nice, France, October 2019, <a href="https://arxiv.org/abs/1908.04011"><strong>[Paperlink]</strong></a>, <a href="https://github.com/Wangt-CN/MTFN-RR-PyTorch-Code"><strong>[Code]</strong></a></em><br>
        <em><strong><font color="#a82e2e">(Oral Presentation, 4.96% acceptance rate)</font></strong></em> <br>
        <em>Area: Visual and Language, Image-text matching</em> <br>
        <p></p>
        <p>In this paper, we propose a novel framework for image-text matching that achieves remarkable matching performance with acceptable model complexity and much less time consuming. </p>
      </td>
    </tr>
   </table>




    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/CASC1.png'  width="200" height="130">
      </td>
      <td valign="top" width="75%">
      <!--
	  <a href="#">
            <papertitle>Cross-stream Selective Networks for Action Recognition</papertitle>
	  </a>
	  <br>
     -->
	 
	 
      <strong>Cross-Modal Attention with Semantic Consistence for Image-Text Matching</strong><br>
	  <a href="https://interxuxing.github.io/">Xing Xu*</a>,
      <strong>Tan Wang*</strong>,
	  <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
	  Lin Zuo,
      <a href="http://cfm.uestc.edu.cn/~fshen/">Fumin Shen</a>,
      <a href="http://cfm.uestc.edu.cn/~shenht/">Heng Tao Shen</a> <br>
	      <em>IEEE Transactions on Neural Networks and learning systems, <strong>TNNLS 2020</strong> </em> <br>
          <em>Area: Visual and Language, Image-text matching</em> <br>
        <p></p>
        <p>In this paper, we propose a novel hybrid matching approach named Cross-modal Attention with Semantic Consistence (CASC) for image-text matching, which is a joint framework that performs cross-modal attention for local alignment and multi-label prediction for global semantic consistence.</p>
        <!-- <p . </p> -->
      </td>
    </tr>
   </table>




    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/radial-gcn.png'  width="200" height="130">
      </td>
      <td valign="top" width="75%">
	  <strong>Cross-Modal Attention with Semantic Consistence for Image-Text Matching</strong><br>     
    <a href="https://interxuxing.github.io/">Xing Xu*</a>,
	<strong>Tan Wang*</strong>,
    <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
      <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a>,
      <a href="http://cfm.uestc.edu.cn/~shenht/">Heng Tao Shen</a> <br>
      <em>IEEE Transactions on Neural Networks and learning systems, <strong>TNNLS 2020</strong> </em> <br>
      <em>Area: Visual and Language, Image-text matching</em> <br>
        <p></p>
        <p>We propose an innovative answer-centric approach <!--termed Radial Graph Convolutional Network (Radial-GCN)--> to focus on the relevant image regions only to reduce the complexity on VQG task.</p>
      </td>
    </tr>
   </table>


<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Honors & Scholarships</heading>
          <div style="line-height:25px">
          <p>
		  <li> <stronghuge>Outstanding Graduates of Sichuan Province</stronghuge> (Top 1% student),&nbsp; 2020<br/>
          <li> <stronghuge>Outstanding Undergraduate Thesis Award</stronghuge> (Top 2% student),&nbsp; 2020<br/>
          <li> <stronghuge>National Scholarship</stronghuge> (Top 2% student),&nbsp; 2018<br/>
          <li> <stronghuge>National Scholarship</stronghuge> (Top 2% student),&nbsp; 2017<br/>
          <li>  <stronghuge>Tang Lixin Sponsored Elite Scholarship</stronghuge> (Only 60 awardees pre year in UESTC),&nbsp; 2017 <br/>
          <li> <stronghuge>Best Freshman Award</stronghuge> (Top 1 student per year in Department),&nbsp; 2016<br/>
          <li> <stronghuge>Honor Student Scholarship</stronghuge> (Top 10 students per year in Department),&nbsp; 2018<br/>
		  <li> <stronghuge>Outstanding Student Scholarship</stronghuge> (Top 10% student),&nbsp; 2017~2019<br/>
          </p>
          </div>
        </td>
      </tr>
</table>



<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Leadership Experience</heading>
      </td>
      </tr>



      </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/lecture.jpg'  width="195" height="130">
      </td>
      <td valign="top" width="75%">
        <stronghuge>Lecture Group of EE Department</stronghuge> </br>
        <huge><em>Founder & President</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Oct. 2017 - Sep. 2018 </br>
        <p></p>
        <p>
         <li> Organized academic forum, sharing sessions, Q&A meetings more than 30 times, serving over 1000 students on studying and future planing.<br/>
          <li> The team grows to 30 people and won the Outstanding Student Organisation prize in 2018.<br/>
          </p> 
      </td>
    </tr>
   </table>


       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="dachuang_stop()" onmouseover="dachuang_start()" >
      <td width="26%">
        <div class="one">
                <div class="two" id='dachuang_image'><img src='image/dachuang2.png'  width="195" height="130"></div>
                <img src='image/dachuang1.png'  width="195" height="130">
              </div>
      <script type="text/javascript">
                function dachuang_start() {
                  document.getElementById('dachuang_image').style.opacity = "1";
                }
                function dachuang_stop() {
                  document.getElementById('dachuang_image').style.opacity = "0";
                }
                dachuang_stop()
              </script>
      </td>

      <td valign="top" width="75%">
        <stronghuge>Innovative Entrepreneurship Project of UESTC</stronghuge> </br>
        <huge><em>Team Leader</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Sep. 2017 - Mar. 2018 </br>
        <p></p>
        <p>
         <li> This project focus on the pedestrian detection in low-light condition with excellent conclusion. We combine the recent pedestrian detection models with the low-light image enhancement algorithm based on Laplace operator.<br/>
         <li> Responsible for the code implementation and project promotion.<br/>
          </p> 
      </td>
    </tr>
   </table>
          




<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Personal Interests</heading>
          <p>
          <stronghuge>DOTA1</stronghuge>: My first and most playing PC game which accompanied me in my whole middle and high school. And I got about 1350 score  on the '11' Battle Platform Ladder Tournament. :)
          </p>
          <p>
          <stronghuge>Running</stronghuge>: During my college, I offen run a long distance for the pleasure releasing. And I have participated in the Chengdu Shuangyi Marathon in 2018.
          </p>
      </td>
      </tr>





   <p></p><p></p><p></p><p></p><p></p>
   <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=250&t=tt&d=85Rlf3OqLYVhTE6hGEcHnAsDJl6O0EsUp326ZMpLzCI"></script>
   
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="middle"><font size="2">
				This awesome template borrowed from <a href="https://people.eecs.berkeley.edu/~barron/">this guy</a>~
				</tbody></table>
   
  </body>
</html>
